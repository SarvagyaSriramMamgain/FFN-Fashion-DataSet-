{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFN_Fashion_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set the wandb credentials.\n",
        "'''\n",
        "project=\"Fashion_MNIST_best_parameter\"\n",
        "entity=\"sarvagyasrirammamgain\""
      ],
      "metadata": {
        "id": "RtLAodzA1riN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2LvnYqyugbKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_5CsOYjOYhO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class: Layer\n",
        "Definations: Initialize_params, activation_fn\n",
        "'''\n",
        "\n",
        "import numpy as np  # numpy for implementing array operations inside the functions.\n",
        "class Layer:\n",
        "    '''\n",
        "    Method: __init__ constructor for base initializations\n",
        "    Input: no of hidden units of the layer, activation for the layer\n",
        "    Output: None\n",
        "    '''\n",
        "    def __init__(self, hidden_units: int, activation:str=None):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "    '''\n",
        "    Method: intialize_params for initializing the weights and biases of each layer\n",
        "    Input: dimension of the input to the layer, no of hidden neurons in the layer, the initialization type(Random or Xavier)\n",
        "    Output: None\n",
        "    ''' \n",
        "    def initialize_params(self, n_in, hidden_units,init_type):\n",
        "        np.random.seed(2)\n",
        "        if init_type==\"Random\":\n",
        "            self.W = 0.01*np.random.randn(n_in, hidden_units)\n",
        "            self.b = 0.01*np.random.randn(1,hidden_units)\n",
        "\n",
        "        elif init_type==\"Xavier\":\n",
        "            self.W = np.random.randn(n_in, hidden_units) * np.sqrt(2/n_in) \n",
        "            self.b = np.zeros((1, hidden_units))\n",
        "\n",
        "    '''\n",
        "    Method: activation_fn for defining the activation functions and thier derivatives to be used by the layers\n",
        "    Input: The computed pre activation of each layer\n",
        "    Output: calculates the activation value for forward prop or the derivative of the activation for backward prop\n",
        "    '''\n",
        "    def activation_fn(self, z, derivative=False):\n",
        "        '''\n",
        "        Relu activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'relu':\n",
        "            if derivative:\n",
        "                return np.where(z<=0,0,1)\n",
        "            return np.maximum(0, z)\n",
        "        '''\n",
        "        sigmoid activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'sigmoid':\n",
        "            if derivative:\n",
        "                return (1 / (1 + np.exp(-z))) * (1-(1 / (1 + np.exp(-z))))\n",
        "            return (1 / (1 + np.exp(-z)))\n",
        "        '''\n",
        "        tanh activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'tanh':\n",
        "            t=(np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
        "            if derivative:\n",
        "                return (1-t**2)\n",
        "            return t\n",
        "        '''\n",
        "        softmax function and its derivativ for the output layer with 10 neurons.\n",
        "        '''\n",
        "        if self.activation == 'softmax':\n",
        "            if derivative: \n",
        "                exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "                return exp / np.sum(exp, axis=0) * (1 - exp / np.sum(exp, axis=0))\n",
        "            exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "            return exp / np.sum(exp, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Class: Helper\n",
        "Definations: Accruacy function, compute_loss function, create batches function.\n",
        "'''\n",
        "import numpy as np  # numpy for performing array operations.\n",
        "class Helper:\n",
        "    '''\n",
        "    Method: accuracy\n",
        "    Input: the true labels and the predicted proababilities generated by the model.\n",
        "    Output: Returns the accuracy of the model on the data.\n",
        "    '''\n",
        "    def accuracy(self,y,y_hat):\n",
        "        c = np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)\n",
        "        acc = list(c).count(True) / len(c) * 100\n",
        "        return acc\n",
        "\n",
        "    '''\n",
        "    Method: compute_loss\n",
        "    Input: the true label, predicted probabilities, loss_type(SquarredError or CrossEntropy) and the regularization coefficient if any.\n",
        "    Output: Returns the value of the loss (SE or CE) plus the regularization loss if any.\n",
        "    '''\n",
        "    def compute_loss(self,Y, Y_hat,layers,loss_type=\"CrossEntropy\",reg=0):\n",
        "        if loss_type==\"CrossEntropy\":\n",
        "            m = Y.shape[0]\n",
        "            L = -1./m * np.sum(Y * np.log(Y_hat+0.0000000001))\n",
        "        elif loss_type==\"SquaredError\":\n",
        "            L = np.mean((Y- Y_hat)**2)\n",
        "\n",
        "        if reg!=0:\n",
        "            reg_error = 0.0                                                                       \n",
        "            for idx in layers.keys() :\n",
        "              reg_error += (reg/2)*(np.sum(np.square(layers[idx].W))) \n",
        "            L = L + reg_error\n",
        "\n",
        "        return L\n",
        "    \n",
        "    '''\n",
        "    Method: create_batches depending on the batch size for training\n",
        "    Input: the training data (X,y) and the batch size\n",
        "    Ouput: Batches of data for training based on the batch size.\n",
        "    '''\n",
        "    def create_batches(self,x, y, batch_size):\n",
        "        m = x.shape[0]\n",
        "        num_batches = m / batch_size\n",
        "        batches = []\n",
        "        for i in range(int(num_batches+1)):\n",
        "            batch_x = x[i*batch_size:(i+1)*batch_size]\n",
        "            batch_y = y[i*batch_size:(i+1)*batch_size]\n",
        "            batches.append((batch_x, batch_y))\n",
        "        \n",
        "        if m % batch_size == 0:\n",
        "            batches.pop(-1)\n",
        "\n",
        "        return batches\n",
        "    "
      ],
      "metadata": {
        "id": "8HJtRgK3O4Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Clss: Neural_Network\n",
        "Definitions: Constructor __init__, add , forward, backward, GDoptimize, \n",
        "              SGDMoptimize, Nesterovoptimize, RMSpropoptimize, Adamoptimize, \n",
        "              Nadamoptimize, fit, predict\n",
        "'''\n",
        "import numpy as np  # numpy to tackle all array related operations\n",
        "from sklearn.model_selection import train_test_split  # train test split for splitting the train data into further train and validation.\n",
        "class Neural_Network:\n",
        "    '''\n",
        "    Method: __init__ constructor for base initialization of layers, cache and gradients for each layer.\n",
        "    Input: None\n",
        "    Output: None\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.layers = dict()\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "\n",
        "    '''\n",
        "    Method: add, to add the layer objects to the model (object of neural network).\n",
        "    Input: layer dictionary\n",
        "    Output: None\n",
        "    '''    \n",
        "    def add(self, layer):\n",
        "        self.layers[len(self.layers)+1] = layer\n",
        "\n",
        "    '''\n",
        "    Method: forward, for forward propagation of the model.\n",
        "    Input: input data and the initilization type of the W,b's of the layer\n",
        "    Output: Returns the predicted probability distribution after forward propagation\n",
        "    '''\n",
        "    def forward(self, x, init_type=\"Xavier\"):\n",
        "        for idx, layer in self.layers.items():\n",
        "\n",
        "            layer.input = np.array(x, copy=True)\n",
        "            if layer.W is None:\n",
        "                layer.initialize_params(layer.input.shape[-1], layer.hidden_units,init_type)  # initilaize the weights and the biases.\n",
        "\n",
        "            layer.Z = x @ layer.W + layer.b # linear pre activation\n",
        "        \n",
        "            if layer.activation is not None:\n",
        "                layer.A = layer.activation_fn(layer.Z) #applying non-linear activation function\n",
        "                x = layer.A\n",
        "            else:\n",
        "                x = layer.Z\n",
        "            self.cache[f'W{idx}'] = layer.W # storing the weights of the layer\n",
        "            self.cache[f'Z{idx}'] = layer.Z # storing the pre activation values of each layer\n",
        "            self.cache[f'A{idx}'] = layer.A # storing the activation values of each layer.\n",
        "        return x\n",
        "\n",
        "    '''\n",
        "    Method: backward, for backward propagation for generating the gradients for weight updation.\n",
        "    Input: true labels, loss_type, regularization coefficient\n",
        "    Output: None, but save the gradients in the grad dictionary of the model(Neural Network object)\n",
        "    '''\n",
        "    def backward(self, y, loss_type,reg=0):\n",
        "        last_layer_idx = max(self.layers.keys())\n",
        "        m = y.shape[0]\n",
        "        for idx in reversed(range(1, last_layer_idx+1)):  # move from output to inputs\n",
        "            if idx == last_layer_idx:\n",
        "                if loss_type==\"CrossEntropy\":\n",
        "                    self.grads[f'dZ{idx}'] = self.cache[f'A{idx}'] - y  # gradient wrt output layer for cross entropy loss\n",
        "                elif loss_type==\"SquaredError\":\n",
        "                    self.grads[f'dZ{idx}'] = (self.cache[f'A{idx}'] - y) * self.layers[idx].activation_fn(self.cache[f'Z{idx}'], derivative=True) # gradients wrt output layer for squared error loss\n",
        "            else:\n",
        "                self.grads[f'dZ{idx}'] = self.grads[f'dZ{idx+1}'] @ self.cache[f'W{idx+1}'].T *\\\n",
        "                                        self.layers[idx].activation_fn(self.cache[f'Z{idx}'], derivative=True) # gradients directly wrt to the pre-activation for each layer.\n",
        "\n",
        "\n",
        "            self.grads[f'dW{idx}'] = 1 / m * self.layers[idx].input.T @ self.grads[f'dZ{idx}'] + reg*self.layers[idx].W # gradients wrt the weights of each layer\n",
        "            self.grads[f'db{idx}'] = 1 / m * np.sum(self.grads[f'dZ{idx}'], axis=0, keepdims=True)  # gradients wrt the biases of each layer.\n",
        "            \n",
        "            assert self.grads[f'dW{idx}'].shape == self.cache[f'W{idx}'].shape\n",
        "\n",
        "    '''\n",
        "    Method: GDoptimize, basically the vanilla gradient descent\n",
        "    Input: learning_rate, idx indicating the layer index\n",
        "    Output: None, but performs the weight updations wrt the gradients.\n",
        "    '''\n",
        "    def GDoptimize(self, idx, learning_rate=1e-3):\n",
        "        \n",
        "        self.layers[idx].W -= learning_rate * self.grads[f'dW{idx}']  # W update\n",
        "        self.layers[idx].b -= learning_rate * self.grads[f'db{idx}']  # b update\n",
        "\n",
        "    '''\n",
        "    Method: SGDMoptimize, basically the momemtum based gradient descent.\n",
        "    Input: learning_rate, idx, mu - fixed momentum coefficient\n",
        "    Output: None, but weights, biases are updated\n",
        "    '''\n",
        "    def SGDMoptimize(self, idx, learning_rate=1e-3, mu=0.99):\n",
        "        m = dict()\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "\n",
        "        m[f'W{idx}'] = m[f'W{idx}'] * mu - learning_rate * self.grads[f'dW{idx}'] # momentum wrt W\n",
        "        m[f'b{idx}'] = m[f'b{idx}'] * mu - learning_rate * self.grads[f'db{idx}'] # momentum wrt b\n",
        "\n",
        "        self.layers[idx].W += m[f'W{idx}']  # W update\n",
        "        self.layers[idx].b += m[f'b{idx}']  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Nesterovoptimize, nesterov accelerated gradien descent.\n",
        "    Input: learning rate, mu - fixed momentum coefficient, idx of the layer\n",
        "    Output: None, but updates the parameters(W,b)\n",
        "    '''\n",
        "    def Nesterovoptimize(self, idx, learning_rate=1e-3, mu=0.99):\n",
        "        m = dict()\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "\n",
        "        mW_prev =  np.array(m[f'W{idx}'], copy=True)\n",
        "        mb_prev = np.array(m[f'b{idx}'], copy=True)\n",
        "\n",
        "        m[f'W{idx}'] = m[f'W{idx}'] * mu - learning_rate * self.grads[f'dW{idx}'] # moemtum update wrt W\n",
        "        m[f'b{idx}'] = m[f'b{idx}'] * mu - learning_rate * self.grads[f'db{idx}'] # momentum update wrt b\n",
        "        # using the lookaheads\n",
        "        w_update = -mu * mW_prev + (1 + mu) * m[f'W{idx}'] \n",
        "        b_update = -mu * mb_prev + (1 + mu) * m[f'b{idx}']\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: RMSpropoptimize, basicall RMSprop gradient descent.\n",
        "    Input: idx of layer, learning rate, decay rate and epsilon\n",
        "    Output: None, updates the parameters.\n",
        "    '''\n",
        "    def RMSpropoptimize(self, idx, learning_rate=1e-3,decay_rate=0.99, epsilon=1e-8):\n",
        "        v = dict()\n",
        "        for i in self.layers.keys():\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "        # using the learning rate decay\n",
        "        v[f'W{idx}'] = decay_rate * v[f'W{idx}'] + (1 - decay_rate) * self.grads[f'dW{idx}'] **2 \n",
        "        v[f'b{idx}'] = decay_rate * v[f'b{idx}'] + (1 - decay_rate) * self.grads[f'db{idx}'] **2\n",
        "        # update values calculation    \n",
        "        w_update = -learning_rate * self.grads[f'dW{idx}'] / (np.sqrt(v[f'W{idx}'] + epsilon))\n",
        "        b_update = -learning_rate * self.grads[f'db{idx}'] / (np.sqrt(v[f'b{idx}']+ epsilon))\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Adamoptimize, Adam optimizer\n",
        "    Input: idx,steps,learing rate, beta1, beta2 and epsilon\n",
        "    Ouput: None, but updates the parameters\n",
        "    '''\n",
        "    def Adamoptimize(self, idx, steps, learning_rate=1e-3, beta1=0.99, beta2=0.999, epsilon=1e-8): \n",
        "        m = dict()\n",
        "        v = dict()\n",
        "\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "\n",
        "        dW = self.grads[f'dW{idx}']\n",
        "        db = self.grads[f'db{idx}']\n",
        "\n",
        "        # weights\n",
        "        m[f'W{idx}'] = beta1 * m[f'W{idx}'] + (1 - beta1) * dW\n",
        "        v[f'W{idx}'] = beta2 * v[f'W{idx}'] + (1 - beta2) * dW ** 2 \n",
        "        \n",
        "        # biases\n",
        "        m[f'b{idx}'] = beta1 * m[f'b{idx}'] + (1 - beta1) * db\n",
        "        v[f'b{idx}'] = beta2 * v[f'b{idx}'] + (1 - beta2) * db ** 2 \n",
        "\n",
        "        # take timestep into account for bias correction\n",
        "        mt_w  = m[f'W{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_w = v[f'W{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        mt_b  = m[f'b{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_b = v[f'b{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        w_update = - learning_rate * mt_w / (np.sqrt(vt_w) + epsilon)\n",
        "        b_update = - learning_rate * mt_b / (np.sqrt(vt_b) + epsilon)\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Nadamoptimize, nesterov accelerated Adam\n",
        "    Input: idx of layer, steps, learing rate, beat1, beta2, epsilon\n",
        "    Output: None, but updates the parameters.\n",
        "    '''\n",
        "    def Nadamoptimize(self, idx, steps,learning_rate=1e-3, beta1=0.99, beta2=0.999, epsilon=1e-8): \n",
        "        m = dict()\n",
        "        v = dict()\n",
        "\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "        dW = self.grads[f'dW{idx}']\n",
        "        db = self.grads[f'db{idx}']\n",
        "        # weights\n",
        "        m[f'W{idx}'] = beta1 * m[f'W{idx}'] + (1 - beta1) * dW\n",
        "        v[f'W{idx}'] = beta2 * v[f'W{idx}'] + (1 - beta2) * dW ** 2 \n",
        "            \n",
        "        # biases\n",
        "        m[f'b{idx}'] = beta1 * m[f'b{idx}'] + (1 - beta1) * db\n",
        "        v[f'b{idx}'] = beta2 * v[f'b{idx}'] + (1 - beta2) * db ** 2 \n",
        "\n",
        "        # take timestep into account for bias correction\n",
        "        mt_w  = m[f'W{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_w = v[f'W{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        mt_b  = m[f'b{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_b = v[f'b{idx}'] / (1 - beta2 ** steps)\n",
        "        # accelerated momentum incorporation into adam\n",
        "        w_update = - learning_rate / (np.sqrt(vt_w) + epsilon) * (beta1 * mt_w + (1 - beta1) *  dW / (1 - beta1 ** steps))\n",
        "        b_update = - learning_rate / (np.sqrt(vt_b) + epsilon) * (beta1 * mt_b + (1 - beta1) *  db / (1 - beta1 ** steps))\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  #b update\n",
        "            \n",
        "    '''\n",
        "    Method: fit, used to train the model by combining forward_prop, back_prop and gradient descent weight updation.\n",
        "    Input: Training data, batch_size, epochs, learning rate, optimizer to use, val_split factor, initialization type of the weights and biases, loss type, and the regularization coefficient\n",
        "    Output: None, but performs the training of the model \n",
        "    '''\n",
        "    def fit(self, x_train, y_train,batch_size=32,epochs=500, learning_rate=1e-3, optimizer=\"GD\",val_split=0.1,init_type=\"Xavier\",loss_type=\"CrossEntropy\",reg=0):\n",
        "        train_accs = [] #stores the training accuracy for each epoch\n",
        "        val_accs = [] #stores the validation accuracy after each epoch\n",
        "        help=Helper() #creating a object of the Helper class for helper functions\n",
        "        \n",
        "        '''Initializations'''\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.init_type=init_type\n",
        "        self.reg=reg\n",
        "        self.loss_type=loss_type\n",
        "\n",
        "        '''Splitting the training data into train and val data based on the val_split value''' \n",
        "        x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=val_split,stratify=y_train,random_state=42)\n",
        "\n",
        "        '''Training Cycle'''\n",
        "        for i in range(1, self.epochs+1):\n",
        "            print(f'Epoch {i}')\n",
        "            batches = help.create_batches(x_train, y_train, batch_size) # create batches based on the batch size\n",
        "            epoch_loss = []\n",
        "            steps = 0 #count the steps in each epoch\n",
        "            \n",
        "            for x, y in batches:\n",
        "                steps += 1\n",
        "                '''Forward Propagation'''\n",
        "                preds = self.forward(x,self.init_type)\n",
        "   \n",
        "                '''backward propagation'''\n",
        "                self.backward(y,self.loss_type,self.reg)\n",
        "                \n",
        "                '''update weights and biases of each layer using the corresponding optimizer'''\n",
        "                for idx in self.layers.keys():\n",
        "                    if self.optimizer ==\"GD\":\n",
        "                        self.GDoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"SGDM\":\n",
        "                        self.SGDMoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Nesterov\":\n",
        "                        self.Nesterovoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"RMSprop\":\n",
        "                        self.RMSpropoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Adam\":\n",
        "                        self.Adamoptimize(idx, steps, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Nadam\":\n",
        "                        self.Nadamoptimize(idx, steps, learning_rate=self.learning_rate)\n",
        "                \n",
        "            '''Predict with network on x_train'''\n",
        "            train_preds = self.forward(x_train)\n",
        "            train_loss = help.compute_loss(y, preds,self.layers,self.loss_type,self.reg)\n",
        "            train_acc=help.accuracy(train_preds,y_train)\n",
        "            train_accs.append(train_acc)\n",
        "            \n",
        "            '''predcit with network on validation data'''\n",
        "            val_preds = self.forward(x_val)\n",
        "            val_acc=help.accuracy(val_preds,y_val)\n",
        "            val_accs.append(val_acc)\n",
        "            val_loss = help.compute_loss(y_val, val_preds,self.layers,self.loss_type,self.reg)\n",
        "\n",
        "            print(f'Train Loss:{train_loss} Train Acc: {train_acc} Val Acc: {val_acc} Val Loss: {val_loss}')  # printing the losses and accuracy after each epoch  \n",
        "            '''Wandb logging values of Train accuracy, Train loss, val accuracy and val loss'''\n",
        "            wandb.log(\n",
        "        {\"Train/Loss\": train_loss, \"Train/Accuracy\": train_acc, \"Val/Accuracy\": val_acc, \"Val/Loss\":val_loss,\"Epoch\":i})\n",
        "                     \n",
        "\n",
        "    '''\n",
        "    Method: Predict, model predictions on any data\n",
        "    Input: Test data to predict on\n",
        "    Output: predicted probabilities of the model on the test data.\n",
        "    '''\n",
        "    def predict(self,x):\n",
        "        preds=self.forward(x)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "4eYJnhuUO6kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wandb credentials and login"
      ],
      "metadata": {
        "id": "HLgny_PqUT_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Installing wandb and login\n",
        "'''\n",
        "! pip install wandb\n",
        "! wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R6ZQJfoO7KV",
        "outputId": "3b4f8884-e99c-404d-a676-d7bc73f5f091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 11.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 38.7 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=d2ab2ab85300296fba3ed2590ea6134b0e4271bfd62e786737adc4f78e5a515e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Essential Imports including the dataset library.\n",
        "'''\n",
        "from keras.datasets import fashion_mnist # dataset to work on.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb"
      ],
      "metadata": {
        "id": "JmtZEWSOO8sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Datset loading'''\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "pjsA7fULPHoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1282769e-8ddf-4f9a-a7d0-a720f91821ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Date preprcessing'''\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(x_train.shape, x_test.shape)\n",
        "x_train = np.array(x_train/255., dtype=np.float32)\n",
        "x_test = np.array(x_test/255., dtype=np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPgiqanYPKe-",
        "outputId": "86bbad33-f091-4a18-9017-8d5d3d4dca72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Method to make labels into a onehot vector'''\n",
        "def one_hot(Y):\n",
        "    num_labels = len(set(Y))\n",
        "    new_Y = []\n",
        "    for label in Y:\n",
        "        encoding = np.zeros(num_labels)\n",
        "        encoding[label] = 1.\n",
        "        new_Y.append(encoding)\n",
        "    return np.array(new_Y)"
      ],
      "metadata": {
        "id": "JJw-2Xp1PNVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Label conversion to onehot vectors'''\n",
        "y_train = one_hot(y_train)\n",
        "y_test = one_hot(y_test)\n",
        "y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDHoHKJfPOnU",
        "outputId": "e08a1a99-932f-4f5e-eb10-3f1da35bf7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (10000, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Method: train, for sweep in wandb for hyper parameter tuning\n",
        "'''\n",
        "def train():\n",
        "    steps = 0\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 10,\n",
        "        'no_hidden_layer':4,\n",
        "        'learning_rate': 1e-3,\n",
        "        'opt':'adam',\n",
        "        'activation':'tanh',\n",
        "        'batch_size':64,\n",
        "        'size_hidden':128,\n",
        "        'reg':0,\n",
        "        'init_type':'Xavier'\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(project=project, entity=entity,config=config_defaults)\n",
        "    \n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    lr = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    opt = config.opt\n",
        "    acti=config.activation\n",
        "    batch_size = config.batch_size\n",
        "    hidden_size=config.size_hidden\n",
        "    reg=config.reg\n",
        "    init_type=config.init_type\n",
        "    no_hidden_layer=config.no_hidden_layer\n",
        "    if opt==\"gd\":\n",
        "        opt=\"GD\"\n",
        "    elif opt=='adam':\n",
        "      opt=\"Adam\"\n",
        "    elif opt=='rmsprop':\n",
        "      opt=\"RMSprop\"\n",
        "    elif opt=='sgdm':\n",
        "      opt='SGDM'\n",
        "    elif opt=='nadam':\n",
        "      opt=\"Nadam\"\n",
        "    elif opt=='nesterov':\n",
        "      opt=\"Nesterov\"\n",
        "    # Model training here and sweeping the values.\n",
        "    model = Neural_Network()\n",
        "    for i in range(no_hidden_layer):\n",
        "        model.add(Layer(hidden_size, activation=acti))\n",
        "\n",
        "    model.add(Layer(10, activation='softmax'))\n",
        "    print(model.layers)\n",
        "    model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, learning_rate=lr, optimizer=opt,val_split=0.1,init_type=init_type,loss_type=\"CrossEntropy\",reg=reg)"
      ],
      "metadata": {
        "id": "SOMRUNDNPWjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing on the Best parameter set and generaing the Confusion Matrix**"
      ],
      "metadata": {
        "id": "k4NVrYjQp6j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Project initialization\n",
        "'''\n",
        "wandb.init(project=project, entity=entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "ZXgZEqJKp8vi",
        "outputId": "a176f96c-c026-48f7-de11-131f64ec6c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msarvagyasrirammamgain\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220503_184635-c611rcx0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sarvagyasrirammamgain/Fashion_MNIST_best_parameter/runs/c611rcx0\" target=\"_blank\">fallen-field-1</a></strong> to <a href=\"https://wandb.ai/sarvagyasrirammamgain/Fashion_MNIST_best_parameter\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sarvagyasrirammamgain/Fashion_MNIST_best_parameter/runs/c611rcx0?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f596d772950>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Testing the models on Fashion_MNIST data for the best configuration.\n",
        "'''\n",
        "'''\n",
        "Best Hyper parameter set\n",
        "'''\n",
        "epochs = 10\n",
        "acti='tanh'\n",
        "lr = 1e-4\n",
        "batch_size = 64\n",
        "optimizer=\"RMSprop\"\n",
        "init_type=\"Xavier\"\n",
        "loss_type=\"CrossEntropy\"\n",
        "reg=0.0005\n",
        "hidden_size=64\n",
        "no_hidden_layer=4\n",
        "\n",
        "\n",
        "\n",
        "model = Neural_Network()\n",
        "\n",
        "for i in range(no_hidden_layer):\n",
        "        model.add(Layer(hidden_size, activation=acti))\n",
        "\n",
        "model.add(Layer(10, activation='softmax'))\n",
        "print(model.layers)\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, learning_rate=lr, optimizer=optimizer,val_split=0.1,init_type=init_type,loss_type=loss_type,reg=reg)\n",
        "y_prob=model.predict(x_test)"
      ],
      "metadata": {
        "id": "v2Ub_KVGqEfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help=Helper()\n",
        "accuracy=help.accuracy(y_test,y_prob)"
      ],
      "metadata": {
        "id": "x6w1NBImqGMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "UVEtPjAkqH2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] "
      ],
      "metadata": {
        "id": "l4owaMMisWv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_list=[]\n",
        "for i in range(10):\n",
        "    for j in range(len(y_train)):\n",
        "        if y_train[j] == i :\n",
        "            class_list.append(class_type[y_train[j]])\n",
        "            break"
      ],
      "metadata": {
        "id": "su8727oVsXWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "x_test = np.array(x_test/255., dtype=np.float32)"
      ],
      "metadata": {
        "id": "NGtcKZ2kqJAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob=np.empty(np.shape(y_test))\n",
        "for i,x in enumerate(x_test):\n",
        "    y_prob[i]= (model.predict(x)[0]).argmax()"
      ],
      "metadata": {
        "id": "pfxQmfKcqLBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test,y_prob.shape"
      ],
      "metadata": {
        "id": "ALq2xqT3qMfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "confusion amtrix logging\n",
        "'''\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(preds=y_prob, y_true=y_test, class_names=class_list),\"Test Accuracy\": accuracy })"
      ],
      "metadata": {
        "id": "UbapwbCnqOMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YAeHKHsZPty5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}